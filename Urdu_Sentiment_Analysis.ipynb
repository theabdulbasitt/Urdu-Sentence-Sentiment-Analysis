{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Loading dataset**\n"
      ],
      "metadata": {
        "id": "VYu3GX5KIw8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SXK7uHfCDeOy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
        "\n",
        "# Dropping null values in the 'urdu_text' column\n",
        "df.dropna(subset=['urdu_text'], inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stopwords and extra spacing"
      ],
      "metadata": {
        "id": "w9z8WNz-Y8rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words\n",
        "stopwords = [\n",
        "    'اور', 'کی', 'ہے', 'میں', 'کو', 'کے', 'تھا', 'تھے', 'تھی', 'پر',\n",
        "    'ہو', 'کا', 'نہیں', 'بھی', 'جو', 'وہ', 'یہ', 'ایک', 'اگر',\n",
        "    'تک', 'لیکن', 'ہم', 'تم', 'اس', 'ان', 'اپنے', 'ہمیں', 'میرے',\n",
        "    'پاس', 'سب', 'وہاں', 'جس', 'صرف', 'تجھے', 'چونکہ', 'تاکہ',\n",
        "    'کچھ', 'تھا', 'تو', 'دو', 'ساتھ', 'کیوں', 'پھر', 'اگر',\n",
        "    'جب', 'جہاں', 'ایسا', 'کسی', 'کس', 'انکا', 'اپنا', 'انکے',\n",
        "    'یہاں', 'کیا', 'نہ', 'پہلے', 'بعد', 'کامیابی', 'ہوا',\n",
        "    'دور', 'جہاں', 'کم', 'زیادہ', 'دوسرے', 'جیسے', 'چاہیے',\n",
        "    'بغیر', 'سوال', 'جواب', 'تاکہ', 'خود', 'اپنے', 'کیا'\n",
        "]\n",
        "\n",
        "\n",
        "# Function to remove stop words\n",
        "def removing_stopwords(text):\n",
        "    if isinstance(text, str):  # Checking for exceptions\n",
        "        words = text.split()\n",
        "        clean_words = [word for word in words if word not in stopwords]\n",
        "        return ' '.join(clean_words)\n",
        "    return text\n",
        "\n",
        "# Applying stopword removal\n",
        "df['cleaned_text'] = df['urdu_text'].apply(removing_stopwords)\n",
        "\n",
        "\n",
        "\n",
        "# Display the cleaned text\n",
        "print(df[['urdu_text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo_B09pMY88t",
        "outputId": "03cb422d-4a56-43c1-984d-c18625170475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           urdu_text  \\\n",
            "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
            "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
            "3                                       نہیں پائین 😎   \n",
            "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0         🤣😂😂 لینے دے میری شادی فسادن ٹھیک کوجی 😐😐😐🤣  \n",
            "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں ...  \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
            "3                                            پائین 😎  \n",
            "4  `` مراد علی شاہ بھیس ڈی جی آئی ایس آئی '' حامد...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing emojis, Hashtag and URLS\n"
      ],
      "metadata": {
        "id": "Ke2321x4ZM4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean social media data\n",
        "def clean_social_media_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove hashtags and mentions\n",
        "    text = re.sub(r'#\\S+|@\\S+', '', text)\n",
        "    # Define a dictionary for common emojis and their sentiment\n",
        "    emoji_dict = {'😊': 'positive', '😢': 'negative'}\n",
        "\n",
        "    # Remove all emojis except the ones in emoji_dict\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)  # This removes all punctuation and emojis\n",
        "\n",
        "    # Replace specified emojis with their sentiments\n",
        "    for emoji, sentiment in emoji_dict.items():\n",
        "        text = text.replace(emoji, sentiment)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the text\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(clean_social_media_text)\n",
        "\n",
        "\n",
        "\n",
        "# Display cleaned text\n",
        "print(df[['urdu_text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfB1a1SPZNiq",
        "outputId": "e91e1c63-a936-4edd-f632-c84efab8f808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           urdu_text  \\\n",
            "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
            "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
            "3                                       نہیں پائین 😎   \n",
            "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0                 لینے دے میری شادی فسادن ٹھیک کوجی   \n",
            "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں  \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
            "3                                             پائین   \n",
            "4      مراد علی شاہ بھیس ڈی جی آئی ایس آئی  حامد میر  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying  and stemming and lemmatization**"
      ],
      "metadata": {
        "id": "wtlemv6RcmJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform basic stemming\n",
        "def simple_stem(word):\n",
        "    # List of common Urdu suffixes\n",
        "    suffixes = ['یں', 'وں', 'ے', 'نے', 'ا', 'ی', 'ت', 'نا', 'گا', 'گی', 'تے', 'ل', 'وں', 'تا', 'تے', 'اس', 'کا']\n",
        "\n",
        "    # Remove suffixes from the word\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "\n",
        "    return word  #return word if no suffix\n",
        "\n",
        "# Function for stemming the entire text\n",
        "def stem_text(text):\n",
        "    stemmed_words = [simple_stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Apply stemming to the cleaned text\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(stem_text)\n",
        "\n",
        "# Display the cleaned and stemmed text\n",
        "print(df[['cleaned_text', 'stemmed_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZADq18oPbt8P",
        "outputId": "922abaea-427c-47fa-f31d-6cd100adfe2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        cleaned_text  \\\n",
            "0                 لینے دے میری شادی فسادن ٹھیک کوجی    \n",
            "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں   \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
            "3                                             پائین    \n",
            "4      مراد علی شاہ بھیس ڈی جی آئی ایس آئی  حامد میر   \n",
            "\n",
            "                                        stemmed_text  \n",
            "0                       لین د میر شاد فسادن ٹھیک کوج  \n",
            "1             چ مہمان کھان سرو کر چڑی چاچ ن دسد آں م  \n",
            "2  کامران خان آپک دن بھریہ زمہ دار لگائ گئ اپوزیش...  \n",
            "3                                              پائین  \n",
            "4            مراد عل شاہ بھیس ڈ ج آئ ایس آئ حامد میر  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple lemmatization dictionary\n",
        "lemmatization_dict = {\n",
        "    'کر': 'کرنا',\n",
        "    'جاتی': 'جاناہ',\n",
        "    'گیا': 'جاناہ',\n",
        "    'کیا': 'کرنا',\n",
        "    'تھا': 'ہونا',\n",
        "    'ہوں': 'ہونا',\n",
        "    'ہیں': 'ہونا',\n",
        "    'تھی': 'ہونا',\n",
        "    'چلے': 'چلنا',\n",
        "    'چلی': 'چلنا',\n",
        "    'دے': 'دینا',\n",
        "    'لکھا': 'لکھنا',\n",
        "    'دیکھا': 'دیکھنا',\n",
        "\n",
        "}\n",
        "\n",
        "# Function for basic lemmatization\n",
        "def lemmatize_word(word):\n",
        "    return lemmatization_dict.get(word, word)  # Return the lemma or the original word\n",
        "\n",
        "# Function for lemmatizing the entire text\n",
        "def lemmatize_text(text):\n",
        "    lemmatized_words = [lemmatize_word(word) for word in text.split()]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Applying lemmatization to the cleaned text\n",
        "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
        "\n",
        "# Display the cleaned and lemmatized text\n",
        "print(df[['cleaned_text', 'lemmatized_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fTov9OCmj61",
        "outputId": "880f4dc5-8080-445b-8920-648a09da63e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        cleaned_text  \\\n",
            "0                 لینے دے میری شادی فسادن ٹھیک کوجی    \n",
            "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں میں   \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
            "3                                             پائین    \n",
            "4      مراد علی شاہ بھیس ڈی جی آئی ایس آئی  حامد میر   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0                لینے دینا میری شادی فسادن ٹھیک کوجی  \n",
            "1  چل مہمانوں کھانا سرو کرنا چڑیل چاچی نوں دسدی آ...  \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
            "3                                              پائین  \n",
            "4       مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import re\n",
        "\n",
        "# Function for manual tokenization\n",
        "def manual_tokenizer(text):\n",
        "    # Use regular expressions to split by spaces and keep punctuation separate\n",
        "    tokens = re.findall(r'\\S+', text)  # Split by whitespace, preserving words\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the cleaned and lemmatized text\n",
        "df['tokens'] = df['lemmatized_text'].apply(manual_tokenizer)\n",
        "\n",
        "# Display lemmatized text and tokens\n",
        "print(df[['lemmatized_text', 'tokens']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpgz2yVH24zX",
        "outputId": "8fe04bc7-80e7-444c-db85-d4fe418170e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     lemmatized_text  \\\n",
            "0                لینے دینا میری شادی فسادن ٹھیک کوجی   \n",
            "1  چل مہمانوں کھانا سرو کرنا چڑیل چاچی نوں دسدی آ...   \n",
            "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
            "3                                              پائین   \n",
            "4       مراد علی شاہ بھیس ڈی جی آئی ایس آئی حامد میر   \n",
            "\n",
            "                                              tokens  \n",
            "0        [لینے, دینا, میری, شادی, فسادن, ٹھیک, کوجی]  \n",
            "1  [چل, مہمانوں, کھانا, سرو, کرنا, چڑیل, چاچی, نو...  \n",
            "2  [کامران, خان, آپکی, دن, بھریہ, زمہ, داری, لگائ...  \n",
            "3                                            [پائین]  \n",
            "4  [مراد, علی, شاہ, بھیس, ڈی, جی, آئی, ایس, آئی, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=1000, min_df=1, max_df=0.95)  # limit to top 1000 features\n",
        "\n",
        "# Apply TF-IDF to lemmatized text\n",
        "tfidf_matrix = tfidf.fit_transform(df['tokens'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF DataFrame\n",
        "print(tfidf_df.head())\n",
        "\"\"\"\n",
        "\n",
        "# Function for manual tokenization\n",
        "def manual_tokenizer(text):\n",
        "    # Use regular expressions to split by spaces and keep punctuation separate\n",
        "    tokens = re.findall(r'\\S+', text)  # Split by whitespace, preserving words\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the cleaned and lemmatized text\n",
        "df['tokens'] = df['lemmatized_text'].apply(manual_tokenizer)\n",
        "\n",
        "# Join tokens back into a string for each row\n",
        "df['tokens_joined'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000, min_df=1, max_df=0.95)  # limit to top 1000 features\n",
        "\n",
        "# Apply TF-IDF to the joined tokens\n",
        "tfidf_matrix = tfidf.fit_transform(df['tokens_joined'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "import pandas as pd\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF DataFrame\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9fyZKpO3tzq",
        "outputId": "bd5ce034-d55d-42d0-b274-0ccc1ee7a4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   100   11  200   22  300  pdm  pti  stateabovestaterejected  \\\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "\n",
            "   westandwithsindhpolice   آئ  ...  یاد  یار  یعنی  یقین   یو  یوتھیا  \\\n",
            "0                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "1                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "2                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "3                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "4                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "\n",
            "   یوتھیوں  یوتھیے  یوں  یہی  \n",
            "0      0.0     0.0  0.0  0.0  \n",
            "1      0.0     0.0  0.0  0.0  \n",
            "2      0.0     0.0  0.0  0.0  \n",
            "3      0.0     0.0  0.0  0.0  \n",
            "4      0.0     0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 1000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a Logistic Regression model\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting sentiment on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print only the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvfAQB8-fxn",
        "outputId": "4f778a23-5500-4d03-bf2d-53911ba53dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_Qb9cv5AGKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}