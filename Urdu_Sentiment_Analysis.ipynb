{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Loading dataset**\n"
      ],
      "metadata": {
        "id": "VYu3GX5KIw8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SXK7uHfCDeOy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
        "\n",
        "# Dropping null values in the 'urdu_text' column\n",
        "df.dropna(subset=['urdu_text'], inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stopwords and extra spacing"
      ],
      "metadata": {
        "id": "w9z8WNz-Y8rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words\n",
        "stopwords = [\n",
        "    'Ø§ÙˆØ±', 'Ú©ÛŒ', 'ÛÛ’', 'Ù…ÛŒÚº', 'Ú©Ùˆ', 'Ú©Û’', 'ØªÚ¾Ø§', 'ØªÚ¾Û’', 'ØªÚ¾ÛŒ', 'Ù¾Ø±',\n",
        "    'ÛÙˆ', 'Ú©Ø§', 'Ù†ÛÛŒÚº', 'Ø¨Ú¾ÛŒ', 'Ø¬Ùˆ', 'ÙˆÛ', 'ÛŒÛ', 'Ø§ÛŒÚ©', 'Ø§Ú¯Ø±',\n",
        "    'ØªÚ©', 'Ù„ÛŒÚ©Ù†', 'ÛÙ…', 'ØªÙ…', 'Ø§Ø³', 'Ø§Ù†', 'Ø§Ù¾Ù†Û’', 'ÛÙ…ÛŒÚº', 'Ù…ÛŒØ±Û’',\n",
        "    'Ù¾Ø§Ø³', 'Ø³Ø¨', 'ÙˆÛØ§Úº', 'Ø¬Ø³', 'ØµØ±Ù', 'ØªØ¬Ú¾Û’', 'Ú†ÙˆÙ†Ú©Û', 'ØªØ§Ú©Û',\n",
        "    'Ú©Ú†Ú¾', 'ØªÚ¾Ø§', 'ØªÙˆ', 'Ø¯Ùˆ', 'Ø³Ø§ØªÚ¾', 'Ú©ÛŒÙˆÚº', 'Ù¾Ú¾Ø±', 'Ø§Ú¯Ø±',\n",
        "    'Ø¬Ø¨', 'Ø¬ÛØ§Úº', 'Ø§ÛŒØ³Ø§', 'Ú©Ø³ÛŒ', 'Ú©Ø³', 'Ø§Ù†Ú©Ø§', 'Ø§Ù¾Ù†Ø§', 'Ø§Ù†Ú©Û’',\n",
        "    'ÛŒÛØ§Úº', 'Ú©ÛŒØ§', 'Ù†Û', 'Ù¾ÛÙ„Û’', 'Ø¨Ø¹Ø¯', 'Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ', 'ÛÙˆØ§',\n",
        "    'Ø¯ÙˆØ±', 'Ø¬ÛØ§Úº', 'Ú©Ù…', 'Ø²ÛŒØ§Ø¯Û', 'Ø¯ÙˆØ³Ø±Û’', 'Ø¬ÛŒØ³Û’', 'Ú†Ø§ÛÛŒÛ’',\n",
        "    'Ø¨ØºÛŒØ±', 'Ø³ÙˆØ§Ù„', 'Ø¬ÙˆØ§Ø¨', 'ØªØ§Ú©Û', 'Ø®ÙˆØ¯', 'Ø§Ù¾Ù†Û’', 'Ú©ÛŒØ§'\n",
        "]\n",
        "\n",
        "\n",
        "# Function to remove stop words\n",
        "def removing_stopwords(text):\n",
        "    if isinstance(text, str):  # Checking for exceptions\n",
        "        words = text.split()\n",
        "        clean_words = [word for word in words if word not in stopwords]\n",
        "        return ' '.join(clean_words)\n",
        "    return text\n",
        "\n",
        "# Applying stopword removal\n",
        "df['cleaned_text'] = df['urdu_text'].apply(removing_stopwords)\n",
        "\n",
        "\n",
        "\n",
        "# Display the cleaned text\n",
        "print(df[['urdu_text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo_B09pMY88t",
        "outputId": "03cb422d-4a56-43c1-984d-c18625170475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           urdu_text  \\\n",
            "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
            "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
            "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0         ğŸ¤£ğŸ˜‚ğŸ˜‚ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£  \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº ...  \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
            "3                                            Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜  \n",
            "4  `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing emojis, Hashtag and URLS\n"
      ],
      "metadata": {
        "id": "Ke2321x4ZM4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean social media data\n",
        "def clean_social_media_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove hashtags and mentions\n",
        "    text = re.sub(r'#\\S+|@\\S+', '', text)\n",
        "    # Define a dictionary for common emojis and their sentiment\n",
        "    emoji_dict = {'ğŸ˜Š': 'positive', 'ğŸ˜¢': 'negative'}\n",
        "\n",
        "    # Remove all emojis except the ones in emoji_dict\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)  # This removes all punctuation and emojis\n",
        "\n",
        "    # Replace specified emojis with their sentiments\n",
        "    for emoji, sentiment in emoji_dict.items():\n",
        "        text = text.replace(emoji, sentiment)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the text\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(clean_social_media_text)\n",
        "\n",
        "\n",
        "\n",
        "# Display cleaned text\n",
        "print(df[['urdu_text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfB1a1SPZNiq",
        "outputId": "e91e1c63-a936-4edd-f632-c84efab8f808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           urdu_text  \\\n",
            "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
            "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
            "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0                 Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ   \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº  \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
            "3                                             Ù¾Ø§Ø¦ÛŒÙ†   \n",
            "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying  and stemming and lemmatization**"
      ],
      "metadata": {
        "id": "wtlemv6RcmJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform basic stemming\n",
        "def simple_stem(word):\n",
        "    # List of common Urdu suffixes\n",
        "    suffixes = ['ÛŒÚº', 'ÙˆÚº', 'Û’', 'Ù†Û’', 'Ø§', 'ÛŒ', 'Øª', 'Ù†Ø§', 'Ú¯Ø§', 'Ú¯ÛŒ', 'ØªÛ’', 'Ù„', 'ÙˆÚº', 'ØªØ§', 'ØªÛ’', 'Ø§Ø³', 'Ú©Ø§']\n",
        "\n",
        "    # Remove suffixes from the word\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "\n",
        "    return word  #return word if no suffix\n",
        "\n",
        "# Function for stemming the entire text\n",
        "def stem_text(text):\n",
        "    stemmed_words = [simple_stem(word) for word in text.split()]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Apply stemming to the cleaned text\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(stem_text)\n",
        "\n",
        "# Display the cleaned and stemmed text\n",
        "print(df[['cleaned_text', 'stemmed_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZADq18oPbt8P",
        "outputId": "922abaea-427c-47fa-f31d-6cd100adfe2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        cleaned_text  \\\n",
            "0                 Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ    \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº   \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
            "3                                             Ù¾Ø§Ø¦ÛŒÙ†    \n",
            "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
            "\n",
            "                                        stemmed_text  \n",
            "0                       Ù„ÛŒÙ† Ø¯ Ù…ÛŒØ± Ø´Ø§Ø¯ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬  \n",
            "1             Ú† Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§Ù† Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒ Ú†Ø§Ú† Ù† Ø¯Ø³Ø¯ Ø¢Úº Ù…  \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú© Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø± Ù„Ú¯Ø§Ø¦ Ú¯Ø¦ Ø§Ù¾ÙˆØ²ÛŒØ´...  \n",
            "3                                              Ù¾Ø§Ø¦ÛŒÙ†  \n",
            "4            Ù…Ø±Ø§Ø¯ Ø¹Ù„ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ Úˆ Ø¬ Ø¢Ø¦ Ø§ÛŒØ³ Ø¢Ø¦ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple lemmatization dictionary\n",
        "lemmatization_dict = {\n",
        "    'Ú©Ø±': 'Ú©Ø±Ù†Ø§',\n",
        "    'Ø¬Ø§ØªÛŒ': 'Ø¬Ø§Ù†Ø§Û',\n",
        "    'Ú¯ÛŒØ§': 'Ø¬Ø§Ù†Ø§Û',\n",
        "    'Ú©ÛŒØ§': 'Ú©Ø±Ù†Ø§',\n",
        "    'ØªÚ¾Ø§': 'ÛÙˆÙ†Ø§',\n",
        "    'ÛÙˆÚº': 'ÛÙˆÙ†Ø§',\n",
        "    'ÛÛŒÚº': 'ÛÙˆÙ†Ø§',\n",
        "    'ØªÚ¾ÛŒ': 'ÛÙˆÙ†Ø§',\n",
        "    'Ú†Ù„Û’': 'Ú†Ù„Ù†Ø§',\n",
        "    'Ú†Ù„ÛŒ': 'Ú†Ù„Ù†Ø§',\n",
        "    'Ø¯Û’': 'Ø¯ÛŒÙ†Ø§',\n",
        "    'Ù„Ú©Ú¾Ø§': 'Ù„Ú©Ú¾Ù†Ø§',\n",
        "    'Ø¯ÛŒÚ©Ú¾Ø§': 'Ø¯ÛŒÚ©Ú¾Ù†Ø§',\n",
        "\n",
        "}\n",
        "\n",
        "# Function for basic lemmatization\n",
        "def lemmatize_word(word):\n",
        "    return lemmatization_dict.get(word, word)  # Return the lemma or the original word\n",
        "\n",
        "# Function for lemmatizing the entire text\n",
        "def lemmatize_text(text):\n",
        "    lemmatized_words = [lemmatize_word(word) for word in text.split()]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Applying lemmatization to the cleaned text\n",
        "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
        "\n",
        "# Display the cleaned and lemmatized text\n",
        "print(df[['cleaned_text', 'lemmatized_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fTov9OCmj61",
        "outputId": "880f4dc5-8080-445b-8920-648a09da63e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        cleaned_text  \\\n",
            "0                 Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ    \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº   \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
            "3                                             Ù¾Ø§Ø¦ÛŒÙ†    \n",
            "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0                Ù„ÛŒÙ†Û’ Ø¯ÛŒÙ†Ø§ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ  \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø±Ù†Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢...  \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
            "3                                              Ù¾Ø§Ø¦ÛŒÙ†  \n",
            "4       Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import re\n",
        "\n",
        "# Function for manual tokenization\n",
        "def manual_tokenizer(text):\n",
        "    # Use regular expressions to split by spaces and keep punctuation separate\n",
        "    tokens = re.findall(r'\\S+', text)  # Split by whitespace, preserving words\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the cleaned and lemmatized text\n",
        "df['tokens'] = df['lemmatized_text'].apply(manual_tokenizer)\n",
        "\n",
        "# Display lemmatized text and tokens\n",
        "print(df[['lemmatized_text', 'tokens']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpgz2yVH24zX",
        "outputId": "8fe04bc7-80e7-444c-db85-d4fe418170e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     lemmatized_text  \\\n",
            "0                Ù„ÛŒÙ†Û’ Ø¯ÛŒÙ†Ø§ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ   \n",
            "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø±Ù†Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢...   \n",
            "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
            "3                                              Ù¾Ø§Ø¦ÛŒÙ†   \n",
            "4       Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
            "\n",
            "                                              tokens  \n",
            "0        [Ù„ÛŒÙ†Û’, Ø¯ÛŒÙ†Ø§, Ù…ÛŒØ±ÛŒ, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†, Ù¹Ú¾ÛŒÚ©, Ú©ÙˆØ¬ÛŒ]  \n",
            "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú©Ø±Ù†Ø§, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†Ùˆ...  \n",
            "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...  \n",
            "3                                            [Ù¾Ø§Ø¦ÛŒÙ†]  \n",
            "4  [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¬ÛŒ, Ø¢Ø¦ÛŒ, Ø§ÛŒØ³, Ø¢Ø¦ÛŒ, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=1000, min_df=1, max_df=0.95)  # limit to top 1000 features\n",
        "\n",
        "# Apply TF-IDF to lemmatized text\n",
        "tfidf_matrix = tfidf.fit_transform(df['tokens'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF DataFrame\n",
        "print(tfidf_df.head())\n",
        "\"\"\"\n",
        "\n",
        "# Function for manual tokenization\n",
        "def manual_tokenizer(text):\n",
        "    # Use regular expressions to split by spaces and keep punctuation separate\n",
        "    tokens = re.findall(r'\\S+', text)  # Split by whitespace, preserving words\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the cleaned and lemmatized text\n",
        "df['tokens'] = df['lemmatized_text'].apply(manual_tokenizer)\n",
        "\n",
        "# Join tokens back into a string for each row\n",
        "df['tokens_joined'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000, min_df=1, max_df=0.95)  # limit to top 1000 features\n",
        "\n",
        "# Apply TF-IDF to the joined tokens\n",
        "tfidf_matrix = tfidf.fit_transform(df['tokens_joined'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "import pandas as pd\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF DataFrame\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9fyZKpO3tzq",
        "outputId": "bd5ce034-d55d-42d0-b274-0ccc1ee7a4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   100   11  200   22  300  pdm  pti  stateabovestaterejected  \\\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0                      0.0   \n",
            "\n",
            "   westandwithsindhpolice   Ø¢Ø¦  ...  ÛŒØ§Ø¯  ÛŒØ§Ø±  ÛŒØ¹Ù†ÛŒ  ÛŒÙ‚ÛŒÙ†   ÛŒÙˆ  ÛŒÙˆØªÚ¾ÛŒØ§  \\\n",
            "0                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "1                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "2                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "3                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "4                     0.0  0.0  ...  0.0  0.0   0.0   0.0  0.0     0.0   \n",
            "\n",
            "   ÛŒÙˆØªÚ¾ÛŒÙˆÚº  ÛŒÙˆØªÚ¾ÛŒÛ’  ÛŒÙˆÚº  ÛŒÛÛŒ  \n",
            "0      0.0     0.0  0.0  0.0  \n",
            "1      0.0     0.0  0.0  0.0  \n",
            "2      0.0     0.0  0.0  0.0  \n",
            "3      0.0     0.0  0.0  0.0  \n",
            "4      0.0     0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 1000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a Logistic Regression model\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting sentiment on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print only the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvfAQB8-fxn",
        "outputId": "4f778a23-5500-4d03-bf2d-53911ba53dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_Qb9cv5AGKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}